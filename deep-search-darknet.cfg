[net]
# use moderate batch size with subdivisions for training stability:contentReference[oaicite:0]{index=0}
batch=64
subdivisions=8
inputs=256
time_steps=576  # support sequences up to 576 time steps
learning_rate=0.001  # initial LR (small to improve precision):contentReference[oaicite:1]{index=1}
burn_in=1000  # warmup iterations for learning rate
policy=poly  # polynomial learning rate decay:contentReference[oaicite:2]{index=2}
power=4
max_batches=1000000  # sufficiently large iterations for convergence:contentReference[oaicite:3]{index=3}
momentum=0.9  # standard momentum:contentReference[oaicite:4]{index=4}
decay=0.0005  # weight decay L2 regularization:contentReference[oaicite:5]{index=5}

[lstm]
batch_normalize=1
output=1024
activation=leaky  # LSTM activation (mitigates vanishing gradients for long-term dependencies):contentReference[oaicite:6]{index=6}

[lstm]
batch_normalize=1
output=1024
activation=leaky

[lstm]
batch_normalize=1
output=1024
activation=leaky

[connected]
output=256
activation=linear  # linear output for numeric prediction

[cost]
type=arith
operations=addition,subtraction,multiplication,division
precision=19
big_number=1
