# [net] Section: Global Training and Data Parameters
# ---------------------------------------------------
# batch               - Number of samples processed in one batch.
# subdivisions        - Number of mini-batches per batch (mini_batch = batch/subdivisions).
# width, height       - For image data: all images will be resized to 416x416.
# channels            - Number of image channels (e.g., 3 for RGB).
# inputs              - Dimensionality for non-image data (letters, prices, etc.).
# max_chart_loss      - Maximum loss value displayed in chart.png.
# Contrastive loss parameters and Data augmentation options are also available.
# Optimizer parameters (momentum, decay, learning_rate, burn_in, etc.) control training.
[net]
subdivisions=1
inputs=256
batch=1
momentum=0.9
decay=0.001
max_batches=2000
time_steps=1
learning_rate=0.1
policy=steps
steps=1000,1500
scales=.1,.1

# Three RNN layers with batch normalization and leaky activation.
[rnn]
batch_normalize=1
output=1024
hidden=1024
activation=leaky

[rnn]
batch_normalize=1
output=1024
hidden=1024
activation=leaky

[rnn]
batch_normalize=1
output=1024
hidden=1024
activation=leaky

# A connected (fully connected) layer to reduce dimensionality, using leaky activation.
[connected]
output=256
activation=leaky

# Softmax layer to convert outputs into probability distributions.
[softmax]

# [cost] Section: Custom Arithmetic Loss Function with Engineering Calculator Functions
# ----------------------------------------------------------------------------------------
# This custom cost layer (type "arith") computes the loss based on a set of high-precision
# arithmetic and scientific operations that mimic an engineering calculator. In addition to
# the basic operations (addition, subtraction, multiplication, division), it supports:
#
#   Exponentiation, Square root, Cube root, EXP (scientific notation entry),
#   Natural log (ln), Common log (log), Trigonometric functions (sin, cos, tan),
#   Inverse trigonometric functions (asin, acos, atan), Hyperbolic functions (sinh, cosh, tanh),
#   Factorial, Permutations (nPr), Combinations (nCr), Percentage, Reciprocal,
#   and Parentheses for grouping operations.
#
# The following parameters are defined:
#
#   precision=100   - Perform computations with 100-digit accuracy.
#   big_number=1    - Enable support for operations on very large numbers (e.g., 200-digit numbers).
#   operations      - A comma-separated list of supported operations.
[cost]
type=arith
operations=addition,subtraction,multiplication,division,exponentiation,square_root,cube_root,exp,ln,log,sin,cos,tan,asin,acos,atan,sinh,cosh,tanh,factorial,permutations,combinations,percentage,reciprocal,parentheses
precision=100
big_number=1




chinese

# [net] Section: Global Training and Data Parameters
# ---------------------------------------------------
# batch=1              - Number of samples processed in one batch.
# subdivisions=1       - Number of mini-batches per batch (mini_batch = batch/subdivisions).
# inputs=256           - Dimensionality for non-image data (e.g., letters, prices).
# momentum=0.9         - Momentum for the optimizer (controls contribution of past gradients).
# decay=0.001          - Weight decay for regularization.
# max_batches=2000     - Maximum number of training iterations.
# time_steps=1         - For sequential data, number of time steps to process per iteration.
# learning_rate=0.1    - Initial learning rate.
# policy=steps         - Learning rate scheduling policy.
# steps=1000,1500      - Iterations at which to change the learning rate.
# scales=.1,.1         - Multiplicative factors for learning rate at each step.
[net]
subdivisions=1
inputs=256
batch=1
momentum=0.9
decay=0.001
max_batches=2000
time_steps=1
learning_rate=0.1
policy=steps
steps=1000,1500
scales=.1,.1

# Three RNN layers with batch normalization and leaky activation
[rnn]
batch_normalize=1
output=1024
hidden=1024
activation=leaky

[rnn]
batch_normalize=1
output=1024
hidden=1024
activation=leaky

[rnn]
batch_normalize=1
output=1024
hidden=1024
activation=leaky

# A connected (fully connected) layer to reduce dimensionality,
# using leaky activation.
[connected]
output=256
activation=leaky

# Softmax layer to convert outputs into probability distributions.
[softmax]

# [cost] Section: Custom Arithmetic Loss Function
# -------------------------------------------------
# This custom cost layer (type "arith") is designed to compute losses based on 
# arithmetic operations on big numbers. It supports:
#   - addition
#   - subtraction
#   - multiplication
#   - division
# with high precision.
#
# precision=19      - Number of decimal places for float arithmetic.
# big_number=1      - Enables support for operations on very large numbers (e.g., 200-digit numbers).
# operations=...    - Comma-separated list of arithmetic operations to include in the loss.
[cost]
type=arith
operations=addition,subtraction,multiplication,division
precision=19
big_number=1
