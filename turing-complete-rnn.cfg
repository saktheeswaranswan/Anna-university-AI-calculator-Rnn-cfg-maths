# Optimized RNN for high-precision arithmetic
[net]
batch=64
subdivisions=8
learning_rate=0.001
burn_in=1000
policy=poly
power=4
max_batches=1000000
time_steps=576
inputs=256
momentum=0.9
decay=0.0005

# LSTM layers replacing RNN blocks
[lstm]
batch_normalize=1
output=1024
activation=leaky

[lstm]
batch_normalize=1
output=1024
activation=leaky

# Fully connected output layer
[connected]
output=256
activation=linear

# Cost layer for arithmetic operations with specified precision
[cost]
type=arith
operations=addition,subtraction,multiplication,division
precision=19
big_number=1
